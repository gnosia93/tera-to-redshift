## Panel Interviews ##

3. Panel Interviews - three to five 60-minute interviews
You’ll meet with several Snowflake team members during the panel interview stage. This round typically includes a series of meetings consisting of technical, expertise, system design, behavioral, and collaboration interviews. A 30-minute Tech Talk presentation may be included depending on the level and role you are interviewing for. 

 - [Teradata SQL 변환 가이드](https://cloud.google.com/bigquery/docs/migration/teradata-sql?hl=ko)
 - https://www.datafold.com/resources/redshift-to-snowflake-migration 




## [Snowflake Interview Tip](https://interviewing.io/snowflake-interview-questions) ##

Knowing Snowflake isn't required, knowing good architecture patterns for big data, data pipelines, medallion architecture, etc... is required.

Soft skills are huge, as it's a consulting role, so make sure you're ready for questions and for people to question the assertions/decisions you're discussing.

The more satellite material you know (vendor techs, integrations, connectors, languages used [Python, SQL, Java, Scala, Javascript], the objects themselves, etc...) the better, as it makes it easier to talk intelligently about what you're trying to do.

Certs would go a LONG way, but most SA's come in w no Snowflake cert, as I understand it.

Just keep in mind what they're looking for. People to help architect good Snowflake accounts for clients. People who can do that don't always know Snowflake in-and-out. More often, they know pipelines and data well, and learn how to do those same things within Snowflake. Snowflake will teach you, and help you get your cert. You just need to show them your experience will make that easy and quick.


*
Mine wasn’t too difficult, talking about typical data structures and things like partitions, compression, etc. Nothing code related until I took a the tech test which was still fairly basic to intermediate SQL.

*
I had an exploratory interview with a sales engineering mgr that I knew. She didn’t have any jobs just wanted to see my skill set. Pretty basic sql and data warehousing / modeling questions. Did not cover any python.


* clustering - https://docs.snowflake.com/en/user-guide/tables-clustering-micropartitions


## SQL / Tuning ##


## [데이터베이스 이론](https://velog.io/@heyksw/CS-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4) ##

* 정규화(Normalization) - 입력, 갱신, 삭제이상 방지, 데이터 중복방지, ER 모델에서 사용
  - 1 정규화(No Repeating Grouop)
  - 2 정규화(Whole Key Dependent)
  - 3 정규화(Non-Key Independent)

* [Key](https://jaehee1007.tistory.com/128)
  - Natural Key 주민등록번호, 이메일 주소처럼 비즈니스 적으로 데이터를 구분하는 구분자.
  - Surrogate Key(대체키) Auto Increment 에 해당하는 임의의 seiral 한 값.
  - PK 로 Natural Key 를 사용할지, Surrogate Key를 사용할지는 그떄 그때 다르다.   

  Every join between dimension and fact tables in the data warehouse should be based on meaningless integer surrogate keys. You should avoid using a natural key as the dimension table’s primary key.

  A "surrogate key" is a unique identifier generated by a database system, used as a primary key for a table, that has no inherent meaning within the data itself and is created specifically to uniquely identify each row, often when a natural key (derived from existing data attributes) is not suitable or reliable for that purpose; essentially, it's an artificial key created to simplify data management and relationships between tables

  A "natural key" in a database refers to an attribute or set of attributes that already exist within the data and can uniquely identify a record, essentially acting as a meaningful identifier based on real-world characteristics, like a person's Social Security Number or a product's SKU number, rather than a system-generated identifier; it's also sometimes called a "business key" or "domain key" because it reflects the business domain where the data originates from



* 제약조건
  - Primary
  - Foreignkey
  - NOT NULL
  - CHECK
  - UNIQUE

* Join
  - Inner Join
  - Outer Join
  - Semi Join
  - Anti Join
  - Catesian Join (Cross Join)
  - Natural Join

* ACID
* TX Isolation Level
  - Uncommitted Read (변경중인 데이터도 본다)
  - Commited Read (변경이 완료된 데이터를 본다)
  - Repeatable Read (트랜잭션 마다 번호를 부여, 하나의 트랜잭션은 부여받은 숫자 보다 작은 트래잭션들의 데이터를 반영한다.)
  - Serializable (동시성 자체가 없다) 


* Dimension Tables for Descriptive Context      
  Dimension tables are integral companions to a fact table. The dimension tables contain the textual context associated with a business process measurement event. They describe the “who, what, where, when, how, and why” associated with the event.
As illustrated in Figure 1-3, dimension tables often have many columns or attributes. It is not uncommon for a dimension table to have 50 to 100 attributes; although, some dimension tables naturally have only a handful of attributes. Dimension tables tend to have fewer rows than fact tables, but can be wide with many large text columns. Each dimension is defined by a single primary key (refer to the PK notation in Figure 1-3), which serves as the basis for referential integrity with any given fact table to which it is joined.

* Fact Tables for Measurements    
  The fact table in a dimensional model stores the performance measurements resulting from an organization’s business process events. You should strive to store the low-level measurement data resulting from a business process in a single dimen- sional model. Because measurement data is overwhelmingly the largest set of data, it should not be replicated in multiple places for multiple organizational functions around the enterprise. Allowing business users from multiple organizations to access a single centralized repository for each set of measurement data ensures the use of consistent data throughout the enterprise.








* DW Performance Tunning
  - Partitioning
  - Columnar rather than row wise
  - Materialized View
  - MPP Architecture
  - CTE (with clause which is caching results in memory)
  - In case of oracle, use bitmap index, for snowflake micro partition.
  - select specific column only due to columnar stoage.
  - Avoid external deformation of the column used in where comparison condition of SQL query.

* The CAP theorem says that a distributed system can deliver on only two of three desired characteristics:
  - Consistency, Availability and Partition Tolerance.

* [SCD (Slowly Changing Dimension)](https://go-for-data.tistory.com/entry/%EB%8A%90%EB%A6%B0-%EB%B3%80%EA%B2%BD-%EC%B0%A8%EC%9B%90-Slowly-Changing-DimensionSCD)  : DW 디멘전 이력관리 매커니즘
  - Type 0 No upate in DW, DW has old value but OLTP has current value
  - Type 1 Retain the latest value in both DW and OLTP
  - Type 2 Append New record
  - Type 3 Partialily Update.(Type --> Previous Type / Current Type)
  
* [Data Pipeline](https://www.ibm.com/kr-ko/topics/data-pipeline)
  - Batch (ETL) .. CDC
  - Streaming  
  - [What Data Pipeline Architecture should I use?](https://cloud.google.com/blog/topics/developers-practitioners/what-data-pipeline-architecture-should-i-use/?hl=en)

* Data Quality
  
  DW data quality checks" refers to the process of verifying the accuracy, completeness, and consistency of data within a data warehouse (DW) by performing various tests and validations to identify and address potential errors before the data is used for analysis and reporting; this includes checks for missing values, duplicate records, data type correctness, range limitations, and consistency across different data sources.    
  Key aspects of DW data quality checks:
   - Data profiling:
     Analyzing data characteristics to identify patterns, anomalies, and potential quality issues. 
   - Data validation:
     Comparing data against predefined rules and constraints to ensure accuracy and consistency. 
   - Data cleansing:
     Identifying and correcting errors, inconsistencies, and missing values. 
   - Data standardization:
     Ensuring data is formatted and represented consistently across different sources.

@@@ Data Quality.   
Not Null (Missing Value)   
Unique (Not duplicate)   
Value Format   
Value Range   
Freshness  
Referential Integrity.   
@@@   



* [Data Warehouse Best Practices](https://peliqan.io/blog/data-warehouse-best-practices/)
  - [Data Lineage](https://bitnine.tistory.com/m/554?category=1050165)
    ; What process it goes through and where is it being used ? This helps to increase consistency and accuracy of DW 
  - [IBM - What is data lineage](https://www.ibm.com/topics/data-lineage)

* Data Cleansing   
  Data cleaning, also known as data cleansing or scrubbing, is the process of fixing or removing incorrect, incomplete, or duplicate data from a dataset. The goal of data cleaning is to improve the quality and usability of data, which can lead to more accurate and reliable insights. 

  Here are some common data issues that can arise:
  - Duplication: Data from different sources can overlap or exist in multiple formats. 
  - Inconsistent data: Data can be mislabeled, misformatted, or have inconsistent values or definitions. 
  - Incomplete data: Data can be missing fields or entries. 
  - Outdated data: Data can be out of date.    
  Data cleaning is important because inaccurate data can lead to flawed business decisions and missed opportunities. It's especially important in data-intensive industries like retail and financial services, but it applies to organizations across the board





## [DW Architecture](https://www.thoughtspot.com/data-trends/data-modeling/data-warehouse-architecture) ## 

![](https://github.com/gnosia93/alibaba-interview/blob/main/images/dw-architecture.png)

- [CDC Vs ETL — When to choose](https://medium.com/@prabhakaran_arivalagan/cdc-vs-etl-when-to-choose-ded644b6cc32)

## greenplum ##

![](https://github.com/gnosia93/alibaba-interview/blob/main/images/greenplum-archi.png)


## Redshift ##

* https://aws.amazon.com/blogs/big-data/what-to-consider-when-migrating-data-warehouse-to-amazon-redshift/ 
* [AWS re:Invent 2020: Deep dive on best practices for Amazon Redshift](https://www.youtube.com/watch?v=13iIj34nkQE)

- 테이블이 몇개이고, 테이블별 용량은, 레코드 건수는 ?
- AMP 가 몇대가 있는지 ?
- 네트워크 아웃바운드 가용 용량은 ?
- 현행 고객의 문제점은 ? 
- 프로시저는 있었나 ?
- ETL 은 인포메티가 그대로 사용했나 ?


## teradata ##

![](https://github.com/gnosia93/alibaba-interview/blob/main/images/teradata-archi.png)

* Node − It is the basic unit in Teradata System. Each individual server in a Teradata system is referred as a Node. A node consists of its own operating system, CPU, memory, own copy of Teradata RDBMS software and disk space. A cabinet consists of one or more Nodes.

* Parsing Engine − Parsing Engine is responsible for receiving queries from the client and preparing an efficient execution plan. The responsibilities of parsing engine are −
   - Receive the SQL query from the client
   - Parse the SQL query check for syntax errors
   - Check if the user has required privilege against the objects used in the SQL query
   - Check if the objects used in the SQL actually exists
   - Prepare the execution plan to execute the SQL query and pass it to BYNET
   - Receives the results from the AMPs and send to the client

* Message Passing Layer − Message Passing Layer called as BYNET, is the networking layer in Teradata system. It allows the communication between PE and AMP and also between the nodes. It receives the execution plan from Parsing Engine and sends to AMP. Similarly, it receives the results from the AMPs and sends to Parsing Engine.

* Access Module Processor (AMP) − AMPs, called as Virtual Processors (vprocs) are the one that actually stores and retrieves the data. AMPs receive the data and execution plan from Parsing Engine, performs any data type conversion, aggregation, filter, sorting and stores the data in the disks associated with them. Records from the tables are evenly distributed among the AMPs in the system. Each AMP is associated with a set of disks on which data is stored. Only that AMP can read/write data from the disks.

* BTEQ utility is a powerful utility in Teradata that can be used in both batch and interactive mode. It can be used to run any DDL statement, DML statement, create Macros and stored procedures. BTEQ can be used to import data into Teradata tables from flat file and it can also be used to extract data from tables into files or reports.
* 파일 추출: BTEQ, Fast Export 또는 TPT(Teradata Parallel Transporter)를 통해 일반적으로 CSV 형식의 Teradata 테이블에서 플랫 파일로 데이터를 추출합니다. TPT는 데이터 처리량 측면에서 가장 효율적이므로 가능한 한 항상 TPT를 사용합니다.
* 인덱스, containt 가 있고, 테이블 해시방식으로 샤딩해서 저장하고, row wise 로 데이터를 저장한다.
* 시스템 카탈로그 테이블이 있어서 각종 메타 정보를 조회할 수 있다.

### data integration ###

* [oracle to teradata synchronization /w ogg](https://www.oracle.com/webfolder/technetwork/tutorials/obe/fmw/goldengate/11g/ggs_sect_config_winux_ora_to_ux_tera.pdf)





## teradata to redshift ##
* [AK 프라자 사례](https://aws.amazon.com/ko/solutions/case-studies/akplaza/)
  
@@@

인터넷 쇼핑몰과 백화점을 운영하는 업체이다 (AK 플라자)

1. 오라클은 on-Prem 에 있고 DW 는 테라데이터로 되어 있다.
2. DW 동기화는 ogg ?
3. on-prem 에 있는 테라데이터를 redshift 로 전환한다.
4. 어플리케이셔은 AWS 에 DB 는 on-prem 에 있다 (DX 연결)
5. 오라클과 redshift 는 DMS 로 동기화한다.
6. 테라데이터는 ftp 서버에 있는 파일을 teql 를 이용하여 upload 했다 --> s3 이벤트 구조 필요. 
7. staging layer / dw layer / presentation layer 간의 ETL 은 procedure 로 작성한다 (glue 를 쓰지않음)

- tech condiseration
  데이터 용량은 ?   
  레드쉬프트 몇개의 노드로 할 것인가? 어떤 타입    
  dist key / sort key 는 어떻게 할것인가?
  BI 툴은 무엇인가?  
  테이블은 몇개이고, 제일 복잡한 업무는
  start schema 를 사용중인가 ?
  등등.  
@@@

### AS-IS ###
아래의 고객 환경에 맞는 최적의 마이그레이션 방안이 필요함
- 메인 DW DB: Teradata 12.0
- ETL : [Informatica Powercenter](https://www.javatpoint.com/informatica-powercenter)
- BI : Hyperion (버전 정보 확인 필요)
- 데이터 사이즈: 약 2 TB

![](https://github.com/gnosia93/alibaba-interview/blob/main/images/tera-to-redshift-1.png)

- The supported version of Informatica PowerCenter for Amazon Redshift is PowerExchange for Amazon Redshift 10.2 (released in 2017) or late
- [Installing and Configuring PowerCenter 10.2 in the AWS Cloud](https://docs.informatica.com/data-integration/powercenter/h2l/1318-installing-and-configuring-powercenter-10-2-in-the-aws-clou/installing-and-configuring-powercenter-10-2-in-the-aws-cloud.html)



### TO-BE ###
![](https://github.com/gnosia93/alibaba-interview/blob/main/images/teradata-mig.png)

* 테라데이터 isolation level 을 알아봐야 한다.
* 작업을 시작하기 전에 테라데이터의 ETL 을 중지한다.
* 트랜잭션 테이블(fact) 테이블의 데이터 변경은 최대 1달전까지 발생할 수 있다. 마스터(Dimension) 은 수시로 발생.
   - 큰 Fact 테이블에 대해서는 TPT 를 이용하여 병렬로 S3 로 로딩한 후, 레드쉬프트의 load 커맨드를 이용하여 적재한다 
* https://teradataexample.blogspot.com/2016/03/tpt-export-script-how-to-use-tpt-export.html
* [TPT to S3](https://github.com/dro248/easy_tpt)


* [Migration 고려사항 - Teradata Migration to Amazon Redshift](https://prisoft.com/teradata-migration-to-amazon-redshift/)

@@@
POC 체크 사항   
  1. 데이터 마이그레이션 방법론 (데이터 타입 변환)   
  2. 코드 변환 방법  (프로시저와 SQL)
  3. 테이블 설계 (DistKey / SortKey) 및 데이터 분산 방법 설계
  4. SQL 성능 측정.
  5. 실시간 데이터 분석 기능.  
@@@



### 1. 데이터 마이그레이션 ###
* 데이터마이그레이션은 SCT 의 데이터 데이터 에이전트를 활용한다. 데이타의 타입중 변경이 안되는 데이터 부터 먼저 옮겨놓는다. (마스터) 트랜잭셩성 fact 의 경우 대부분이 이력에 해당하므로 수정 보다는 입력이 주류이다..
* 마스터 데이터, 트랜잭션 데이터를 구분해서 update, delete, insert 에 대비한다. 
https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/agents.html#agents.Installing.AgentSettings

### 2. ETL (dagta integration) ###
   - AS-IS    
       테라데이터는 row-wise 로 데이터를 저장하고 인덱스를 지원한다. 데이터들은 레드쉬프트와 동일하게 샤딩되어져 있다.
       오라클 OLTP 에서 ogg 를 이용해서 CDC 로 변경 데이터를 테라데이터에 카피하고 있고, 프로시저를 이용하여 ETL 을 처리한다.
   - TO-BE
       레드쉬프트는 인덱스를 지원하지 않기 때문에, CDC 로 레드쉬프트에 데이터를 붙는 것은 비효율적이다. (update, delete 시 full scan)  
       etl 역시 프로시저로 작성하는 것은 원본 테이블 전체를 읽는 것과 같아서 추천하지 않는다.
    
   oracle --> s3 (staging) --> redshift 구조 (fact/dimen, summary)
   
          dms              etl
   
   * 데이터는 AWS ETL을 통해 Amazon Simple Storage Service(Amazon S3)에 적재해 전처리하고, Amazon Redshift에 통합한 후 기존에 사용 중인 OLAP(Oracle Hyperion)를 통해 분석

   * DMS 설정은 dataformat 은 파퀘이, 날짜 기반(년월일시) 파티셔닝 사용
          - https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html#CHAP_Target.S3.DatePartitioning
          - https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html#CHAP_Target.S3.Configuring



### 3. 마이그레이션 고려사항 ###
  - 기존 환경 분석 (1개월)
  - 이행 (3개월)
    - 인스턴스 타입 선정 및 데이터 볼륨계산
    - 레드쉬프트 스키마 설계 (분산키, 소트키)
    - 이행 연습 (정합성 체크, 다운타임 단축, 어플리케이션 검증, SQL 성능 검증 및 튜닝)
    - 코드성 데이터 수정 (프로시저, UDF, 매크로)
  - ETL 파이프라인 설계 (3개월)
    - CDC
    - ETL
    - Kafka 커넥터
    - 기타 
  - 안정화 (1개월)
    - 성능 모니터링
    - SQL 튜닝(?)
    - 정합성 오류 수정.   
    

### 4. Issue Fixes for Migration Challenges ### 

* We identified the issue in the Schema Conversion Tool (SCT) related to data type length. To resolve this, we modified the string length twice: first to match the source DDL and then to conform to the target DDL requirements.  
* To avoid collation errors, we changed the database/schema to be case-insensitive.
* For large tables exceeding 20GB, we utilized the virtual partitioning option and applied a date column filter to effectively migrate the data.  
* In Windows, we disabled session timeouts to ensure uninterrupted data migration during background processes.
* To reduce the time required for migration, we provisioned three additional high-resource instances, allowing us to migrate data in parallel and meet the project timeline.  We adjusted the minimum and maximum memory values in the SCT configuration file, resulting in an increase in the speed of the data migration process. 


### 5. Utility 사용법 ###

* [Teradata Parallel Transporter(TPT) - TPTEXPORT - Explanation 2022](https://www.youtube.com/watch?v=NSIqogUoBQU)
* [Table Upload with TPT to S3](https://docs.teradata.com/r/Enterprise_IntelliFlex_Lake_VMware/Teradata-Tools-and-Utilities-Access-Module-Reference-20.00/Teradata-Access-Module-for-S3/TPT-Log-Support)


### 6. 참고자료 ###
* [Use virtual partitioning in the AWS Schema Conversion Tool](https://aws.amazon.com/blogs/database/use-virtual-partitioning-in-the-aws-schema-conversion-tool/)

* [Teradata Migration into Amazon Redshift](https://www.ispirer.com/blog/teradata-migration-into-amazon-redshift)

* https://d1.awsstatic.com/events/reinvent/2020/Dive_Deep_into_AWS_Schema_Conversion_Tool_and_AWS_DMS_DAT312.pdf

* [Achieve Scalability and Cost Savings: Migrating Teradata to AWS Redshift Serverless for Transient Workloads](https://www.1cloudhub.com/migrating-from-on-premises-teradata-to-amazon-redshift/#:~:text=Redshift-,Architecture,is%20optimized%20for%20OLAP%20workloads.)


* [Automate Teradata Data Warehouse Migration to Amazon Redshift Cloud Data Warehouse](https://www.youtube.com/watch?v=lciu5XNBqK4)
* [AWS re:Invent 2020: Migrating a legacy data warehouse to Amazon Redshift](https://www.youtube.com/watch?v=hbM4OSYj3zQ)
  - etl 은 redshift 내에서 stored procedure 로 처리한다. 
  - [BTEQ](https://blog.naver.com/zerorae/70088144616)  <--- oracle sqlplus 같은거.
  - [How to use Amazon Redshift Query Editor v2 to Schedule Queries](https://www.youtube.com/watch?v=gTw0XUpO8sw)
  - https://aws.amazon.com/ko/blogs/tech/scheduling-with-query-editor-v2-on-amazon-redshift-serverless/

* [Moving data from Teradata to Amazon Redshift using ETL scripts in AWS Glue](https://www.youtube.com/watch?v=rKSjgkzrbQQ)
  ; gteq 로 text 파일 로딩하는 파이프 라인은 S3 와 glue 로 대체한다. 그런데 날짜별이면 어떻게 처리되는 것인가?   

* [Teradata Migration to AWS and Snowflake](https://www.linkedin.com/pulse/teradata-migration-aws-snowflake-suresh-pala/)

* [teradata vs redshift](https://cdn.hevodata.com/whitepapers/Redshift%20Vs%20Teradata%20-%20An%20In-Depth%20Comparison.pdf)

### migration /w SCT Data Extractor Agents on EC2 ###

* [Data Migration from Teradata to AWS Redshift using AWS SCT](https://medium.com/@er.sanketwagh/data-migration-from-teradata-to-aws-redshift-using-aws-sct-b8d8906f7c88)  
  - AWS SCT with Data Extractor Agents is a great choice if you wish to migrate petabytes scale of data from On-Premise systems to AWS Redshift. It’s fast, powerful, Cloud Native, and very cost-efficient.
  - https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-a-teradata-database-to-amazon-redshift-using-aws-sct-data-extraction-agents.html

* [AWS re:Invent 2019: [REPEAT 1] Migrating your data warehouses to Amazon Redshift (DAT363-R1)](https://www.youtube.com/watch?v=SldtNAonxEk)
![](https://github.com/gnosia93/alibaba-interview/blob/main/images/sct-data-agent.png)

* https://aws.amazon.com/blogs/database/integrating-teradata-with-amazon-redshift-using-the-aws-schema-conversion-tool/




### s3 file to redshift ###

* https://stackoverflow.com/questions/67009644/how-to-import-load-data-from-csv-files-on-s3-bucket-into-redshift-using-aws-glue


* [ETL | Incremental Data Load from Amazon S3 Bucket to Amazon Redshift Using AWS Glue](https://www.youtube.com/watch?v=RGSKeK9xow0)
; s3 에 있는 파일을 update 한후 다시 etl job 을 수행하는 데모이다. 

* [Simplify data ingestion from Amazon S3 to Amazon Redshift using auto-copy](https://aws.amazon.com/blogs/big-data/simplify-data-ingestion-from-amazon-s3-to-amazon-redshift-using-auto-copy/)
; redshift 가 s3에 파일이 새롭게 올라오면 자동 카피하는 방식이다. 


* [ETL with Multiple file processing using s3, Lambda and Glue Job](https://diwamishra21.medium.com/etl-with-multiple-file-processing-using-s3-lambda-and-glue-job-f0c62062fd53)

* [schedule batch jobs /w AWS Data Pipeline](https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift.html)




## lakehouse ##

* [Apache Iceberg & ACID Transactions](https://medium.com/@tglawless/apache-iceberg-acid-transactions-ec9d7b7afff5)



## Integration ##

* [프로토콜 버퍼](https://velog.io/@pdg03092/%ED%94%84%EB%A1%9C%ED%86%A0%EC%BD%9C-%EB%B2%84%ED%8D%BC%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80Protocol-Buffer)
* [카프라 커넥트](https://dytis.tistory.com/74)
* [Create a modern data platform using the Data Build Tool (dbt) in the AWS Cloud](https://aws.amazon.com/blogs/big-data/create-a-modern-data-platform-using-the-data-build-tool-dbt-in-the-aws-cloud/)


## Dbt ##

* [Introduction to DBT (Data Build Tool) | ETL Vs ELT](https://www.youtube.com/watch?v=b2nSMPiXdXk&list=PLc2EZr8W2QIBegSYp4dEIMrfLj_cCJgYA&index=1)
* [How to Install DBT and Set Up a Project, Create Your First dbt Model](https://www.youtube.com/watch?v=1fY1A8SRflI&list=PLc2EZr8W2QIBegSYp4dEIMrfLj_cCJgYA&index=2)
* [DBT Models - How to Build Scalable Data Pipelines with data build tool](https://www.youtube.com/watch?v=cW7KFaos2cw&list=PLc2EZr8W2QIBegSYp4dEIMrfLj_cCJgYA&index=3)
* [DBT Sources, Seeds & Analyses | Data Build Tool Demo with Real World Examples](https://www.youtube.com/watch?v=16Gr2da9YWE&list=PLc2EZr8W2QIBegSYp4dEIMrfLj_cCJgYA&index=4)
* [DBT Tests | Data Build Tool | Singular | Generic | Custom | Prebuild | Source Freshness](https://www.youtube.com/watch?v=OpsQfyrWTs8&list=PLc2EZr8W2QIBegSYp4dEIMrfLj_cCJgYA&index=5)
* [DBT doc blocks | DBT Docs | dbt documentation best practices](https://www.youtube.com/watch?v=Am-gt-HYu84&list=PLc2EZr8W2QIBegSYp4dEIMrfLj_cCJgYA&index=6)
* [Jinja in dbt (Data Build Tool) | Using Jinja functions | Templating | Generating Reusable Code](https://www.youtube.com/watch?v=85El5efB6uw&list=PLc2EZr8W2QIBegSYp4dEIMrfLj_cCJgYA&index=7)
* [Macros in dbt | Data Build Tool | Jinja and macros tutorial](https://www.youtube.com/watch?v=VRTx97llL-A&list=PLc2EZr8W2QIBegSYp4dEIMrfLj_cCJgYA&index=8)
* [Dbt Packages | What is a package in data build tool? | dbt-utils | dbt-expectations | dbt deps](https://www.youtube.com/watch?v=9NbPM5VNeUc&list=PLc2EZr8W2QIBegSYp4dEIMrfLj_cCJgYA&index=9)
* [DBT Materialisations | Data Build Tool | Table Vs View vs Ephemeral](https://www.youtube.com/watch?v=WGEatDC4Nrc&list=PLc2EZr8W2QIBegSYp4dEIMrfLj_cCJgYA&index=10)
* [Dbt Materializations - Incremental Snapshot | data build tool | Slowly Changing Dimension SCD Type 2](https://www.youtube.com/watch?v=bjemdsZibdM&list=PLc2EZr8W2QIBegSYp4dEIMrfLj_cCJgYA&index=11)
* [Productionizing dbt core | Data Build Tool | Airflow | GitHub | Azure DevOps | CI/CD](https://www.youtube.com/watch?v=XhUdKcyYSJc&list=PLc2EZr8W2QIBegSYp4dEIMrfLj_cCJgYA&index=12)


#### 워크샵 ####
* https://catalog.workshops.aws/dbt-cli-and-amazon-redshift/en-US



### Data Warehouse ##

* [dbt를 통한 데이터 웨어하우스 개발 후기](https://medium.com/iotrustlab/data-warehouse-with-dbt-b65be67750e9)
* [dbt로 ELT 파이프라인 효율적으로 관리하기](https://www.humphreyahn.dev/blog/efficient-elt-pipelines-with-dbt#96561dc1-bd2b-420f-8014-5c7b91c32e2f)

### airflow ###
* [DBT와 Airflow 도입하며 마주한 7가지 문제들](https://medium.com/daangn/dbt%EC%99%80-airflow-%EB%8F%84%EC%9E%85%ED%95%98%EB%A9%B0-%EB%A7%88%EC%A3%BC%ED%95%9C-7%EA%B0%80%EC%A7%80-%EB%AC%B8%EC%A0%9C%EB%93%A4-61250a9904ab)



## shell ##

* [$? - 쉘에서 좀전에 실행한 명령어의 결과값](https://etloveguitar.tistory.com/27)


## 참고자료 ##
* [Monitoring & Troubleshooting for AWS Glue | Amazon Web Services](https://www.youtube.com/watch?v=z8C1Vpct73g)
     ; SparkUI, cloudwatch logs, cloudwatch metric 조회 가능하다.
* [Simplifying AWS Glue Job Failure Notifications with SNS: A Step-by-Step Guide](https://jainsaket-1994.medium.com/simplifying-aws-glue-job-failure-notifications-with-sns-a-step-by-step-guide-e2c801c4a2dd)
  
* [Greenplum](https://greenplum.org/introduction-to-greenplum-architecture/)
* [IBM Netizza](https://www.slideshare.net/slideshow/netezza-architecture-and-administration/75312066)

* [The definitive guide to accelerating your Teradata to Redshift Migration](https://assets-global.website-files.com/62ac726757a0ffd07f10a5b1/63bd387fae5fa96dba917dfc_The%20definitive%20guide%20to%20accelerating%20your%20Teradata%20to%20Redshift%20Migration.pdf)

* [Migration - ebook](https://www.agilisium.com/ebooks/the-definitive-guide-to-accelerating-your-teradata-to-redshift-migration)

* [AWS GLUE 로 손쉬운 데이터 전처리 작업하기](https://www.slideshare.net/slideshow/aws-glue-112394474/112394474#38)

* https://stackoverflow.com/questions/76227738/merge-into-with-datalake-on-aws-glue-inserting-rows-instead-of-updating

* [AWS Glue PySpark:Insert records into Amazon Redshift Table](https://www.youtube.com/watch?v=EetkEf359QE)

  
* https://www.tutorialspoint.com/teradata/index.htm

* [VMWare Pro](https://craykorea.tistory.com/143)

